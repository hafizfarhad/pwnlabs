{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73a5669a",
   "metadata": {},
   "source": [
    "# üéØ AI Security Lab: Adversarial Evasion Attacks\n",
    "\n",
    "Welcome to your first AI Security lab! In this hands-on session, you'll learn how to:\n",
    "\n",
    "1. **Understand adversarial examples** - Small perturbations that fool ML models\n",
    "2. **Implement FGSM attacks** - Fast Gradient Sign Method\n",
    "3. **Test on real models** - Attack pre-trained image classifiers\n",
    "4. **Analyze results** - Measure attack effectiveness\n",
    "\n",
    "**üèÜ Goal**: Generate adversarial examples that achieve >90% attack success rate\n",
    "\n",
    "**üìç Flag Location**: Complete all challenges to reveal the flag!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347e1ace",
   "metadata": {},
   "source": [
    "## üìö Step 1: Import Libraries and Setup\n",
    "\n",
    "Let's start by importing the necessary libraries for our adversarial attack lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf0b8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import json\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0463297a",
   "metadata": {},
   "source": [
    "## üß† Step 2: Load Pre-trained Model\n",
    "\n",
    "We'll use a pre-trained ResNet-18 model as our target classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97034d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet-18\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.eval()  # Set to evaluation mode\n",
    "model = model.to(device)\n",
    "\n",
    "# ImageNet preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load ImageNet class labels\n",
    "with open('https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702657f5",
   "metadata": {},
   "source": [
    "## üì∏ Step 3: Load and Classify a Test Image\n",
    "\n",
    "Let's start with a clean image and see how the model classifies it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a671c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample image URL (you can replace with your own)\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/PNG_transparency_demonstration_1.png/280px-PNG_transparency_demonstration_1.png\"\n",
    "\n",
    "def load_image(url):\n",
    "    \"\"\"Load image from URL\"\"\"\n",
    "    response = requests.get(url)\n",
    "    image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    return image\n",
    "\n",
    "def classify_image(image, model, transform, classes, top_k=5):\n",
    "    \"\"\"Classify image and return top-k predictions\"\"\"\n",
    "    # Preprocess image\n",
    "    img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img_tensor)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        top_prob, top_indices = torch.topk(probabilities, top_k)\n",
    "    \n",
    "    # Format results\n",
    "    results = []\n",
    "    for i in range(top_k):\n",
    "        results.append({\n",
    "            'class': classes[top_indices[0][i]],\n",
    "            'confidence': top_prob[0][i].item()\n",
    "        })\n",
    "    \n",
    "    return results, img_tensor\n",
    "\n",
    "# Load and classify original image\n",
    "original_image = load_image(image_url)\n",
    "original_results, original_tensor = classify_image(original_image, model, transform, classes)\n",
    "\n",
    "# Display results\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(original_image)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "class_names = [r['class'] for r in original_results]\n",
    "confidences = [r['confidence'] for r in original_results]\n",
    "plt.barh(class_names, confidences)\n",
    "plt.title('Model Predictions')\n",
    "plt.xlabel('Confidence')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîç Original Image Classification:\")\n",
    "for i, result in enumerate(original_results):\n",
    "    print(f\"{i+1}. {result['class']}: {result['confidence']:.3f}\")\n",
    "\n",
    "# Store the original prediction for later\n",
    "original_class = original_results[0]['class']\n",
    "original_confidence = original_results[0]['confidence']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d065a7",
   "metadata": {},
   "source": [
    "## ‚öîÔ∏è Step 4: Implement FGSM Attack\n",
    "\n",
    "Now comes the exciting part - implementing the Fast Gradient Sign Method (FGSM) attack!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f45b88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    \"\"\"Perform FGSM attack on image\"\"\"\n",
    "    # Get the sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    \n",
    "    # Create adversarial example\n",
    "    perturbed_image = image + epsilon * sign_data_grad\n",
    "    \n",
    "    # Clamp to maintain valid pixel range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    \n",
    "    return perturbed_image\n",
    "\n",
    "def generate_adversarial_example(image_tensor, target_class, model, epsilon):\n",
    "    \"\"\"Generate adversarial example using FGSM\"\"\"\n",
    "    # Set image tensor to require gradients\n",
    "    image_tensor.requires_grad = True\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(image_tensor)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = F.cross_entropy(outputs, target_class)\n",
    "    \n",
    "    # Zero gradients\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Get gradients\n",
    "    data_grad = image_tensor.grad.data\n",
    "    \n",
    "    # Generate adversarial example\n",
    "    adversarial_image = fgsm_attack(image_tensor, epsilon, data_grad)\n",
    "    \n",
    "    return adversarial_image, data_grad\n",
    "\n",
    "print(\"‚úÖ FGSM attack functions defined!\")\n",
    "print(\"üéØ Ready to generate adversarial examples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93989e9a",
   "metadata": {},
   "source": [
    "## üí• Step 5: Launch the Attack!\n",
    "\n",
    "Let's create adversarial examples with different epsilon values and see how they affect the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493f0d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attack parameters\n",
    "epsilons = [0.01, 0.03, 0.05, 0.1, 0.3]\n",
    "target_class = torch.tensor([1]).to(device)  # Target class for attack\n",
    "\n",
    "# Store results\n",
    "attack_results = []\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "\n",
    "for i, epsilon in enumerate(epsilons):\n",
    "    # Generate adversarial example\n",
    "    adv_image, grad = generate_adversarial_example(\n",
    "        original_tensor.clone(), target_class, model, epsilon\n",
    "    )\n",
    "    \n",
    "    # Convert back to PIL image for display\n",
    "    adv_img_display = adv_image.squeeze().cpu().detach()\n",
    "    # Denormalize for display\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    adv_img_display = adv_img_display * std + mean\n",
    "    adv_img_display = torch.clamp(adv_img_display, 0, 1)\n",
    "    \n",
    "    # Get predictions for adversarial example\n",
    "    with torch.no_grad():\n",
    "        adv_outputs = model(adv_image)\n",
    "        adv_probs = F.softmax(adv_outputs, dim=1)\n",
    "        top_prob, top_idx = torch.topk(adv_probs, 1)\n",
    "        \n",
    "    predicted_class = classes[top_idx[0][0]]\n",
    "    confidence = top_prob[0][0].item()\n",
    "    \n",
    "    # Check if attack was successful (changed prediction)\n",
    "    attack_success = predicted_class != original_class\n",
    "    \n",
    "    attack_results.append({\n",
    "        'epsilon': epsilon,\n",
    "        'predicted_class': predicted_class,\n",
    "        'confidence': confidence,\n",
    "        'attack_success': attack_success\n",
    "    })\n",
    "    \n",
    "    # Plot results\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.imshow(adv_img_display.permute(1, 2, 0))\n",
    "    status = \"‚úÖ SUCCESS\" if attack_success else \"‚ùå FAILED\"\n",
    "    plt.title(f'Œµ={epsilon}\\n{predicted_class}\\n{confidence:.3f}\\n{status}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Attack Results Summary:\")\n",
    "print(f\"Original: {original_class} ({original_confidence:.3f})\\n\")\n",
    "\n",
    "successful_attacks = 0\n",
    "for result in attack_results:\n",
    "    status = \"SUCCESS\" if result['attack_success'] else \"FAILED\"\n",
    "    print(f\"Œµ={result['epsilon']}: {result['predicted_class']} ({result['confidence']:.3f}) - {status}\")\n",
    "    if result['attack_success']:\n",
    "        successful_attacks += 1\n",
    "\n",
    "success_rate = (successful_attacks / len(epsilons)) * 100\n",
    "print(f\"\\nüìä Overall Attack Success Rate: {success_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2e2083",
   "metadata": {},
   "source": [
    "## üîç Step 6: Analyze Attack Effectiveness\n",
    "\n",
    "Let's dig deeper into our attack results and understand what makes them effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56670ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot success rate vs epsilon\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "success_rates = [1 if r['attack_success'] else 0 for r in attack_results]\n",
    "plt.plot(epsilons, success_rates, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Epsilon (Œµ)')\n",
    "plt.ylabel('Attack Success')\n",
    "plt.title('Attack Success vs Perturbation Strength')\n",
    "plt.grid(True)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "confidences = [r['confidence'] for r in attack_results]\n",
    "plt.plot(epsilons, confidences, 's-', linewidth=2, markersize=8, color='red')\n",
    "plt.axhline(y=original_confidence, color='blue', linestyle='--', label='Original Confidence')\n",
    "plt.xlabel('Epsilon (Œµ)')\n",
    "plt.ylabel('Model Confidence')\n",
    "plt.title('Model Confidence vs Perturbation Strength')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal epsilon\n",
    "successful_epsilons = [r['epsilon'] for r in attack_results if r['attack_success']]\n",
    "if successful_epsilons:\n",
    "    optimal_epsilon = min(successful_epsilons)\n",
    "    print(f\"üéØ Optimal epsilon (smallest successful): {optimal_epsilon}\")\n",
    "else:\n",
    "    print(\"‚ùå No successful attacks found. Try larger epsilon values!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4559c393",
   "metadata": {},
   "source": [
    "## üèÜ Step 7: Challenge - Achieve High Attack Success\n",
    "\n",
    "**Challenge**: Create adversarial examples that achieve >90% attack success rate!\n",
    "\n",
    "Try different strategies:\n",
    "1. Adjust epsilon values\n",
    "2. Try different target classes\n",
    "3. Experiment with multiple iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564c1ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Implement your attack strategy here!\n",
    "# Try to achieve >90% success rate\n",
    "\n",
    "# Test with finer epsilon range\n",
    "challenge_epsilons = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
    "challenge_results = []\n",
    "\n",
    "print(\"üéØ Challenge: Achieving >90% Attack Success Rate\")\n",
    "print(\"Testing with extended epsilon range...\\n\")\n",
    "\n",
    "successful_attacks = 0\n",
    "for epsilon in challenge_epsilons:\n",
    "    # Generate adversarial example\n",
    "    adv_image, _ = generate_adversarial_example(\n",
    "        original_tensor.clone(), target_class, model, epsilon\n",
    "    )\n",
    "    \n",
    "    # Test the attack\n",
    "    with torch.no_grad():\n",
    "        adv_outputs = model(adv_image)\n",
    "        adv_probs = F.softmax(adv_outputs, dim=1)\n",
    "        top_prob, top_idx = torch.topk(adv_probs, 1)\n",
    "        \n",
    "    predicted_class = classes[top_idx[0][0]]\n",
    "    confidence = top_prob[0][0].item()\n",
    "    attack_success = predicted_class != original_class\n",
    "    \n",
    "    if attack_success:\n",
    "        successful_attacks += 1\n",
    "    \n",
    "    status = \"‚úÖ SUCCESS\" if attack_success else \"‚ùå FAILED\"\n",
    "    print(f\"Œµ={epsilon}: {predicted_class} ({confidence:.3f}) - {status}\")\n",
    "    \n",
    "    challenge_results.append({\n",
    "        'epsilon': epsilon,\n",
    "        'attack_success': attack_success\n",
    "    })\n",
    "\n",
    "# Calculate final success rate\n",
    "final_success_rate = (successful_attacks / len(challenge_epsilons)) * 100\n",
    "print(f\"\\nüìä Final Attack Success Rate: {final_success_rate:.1f}%\")\n",
    "\n",
    "# Check if challenge is completed\n",
    "if final_success_rate >= 90:\n",
    "    print(\"\\nüéâ CHALLENGE COMPLETED!\")\n",
    "    print(\"üèÜ You achieved >90% attack success rate!\")\n",
    "    \n",
    "    # Reveal the flag!\n",
    "    flag = \"PWNTHEPROMPT{92.5_percent_adversarial_success_achieved}\"\n",
    "    print(f\"\\nüö© FLAG: {flag}\")\n",
    "    \n",
    "    print(\"\\nüìù What you learned:\")\n",
    "    print(\"‚Ä¢ How to implement FGSM adversarial attacks\")\n",
    "    print(\"‚Ä¢ The relationship between perturbation strength and attack success\")\n",
    "    print(\"‚Ä¢ How small pixel changes can fool state-of-the-art models\")\n",
    "    print(\"‚Ä¢ The importance of adversarial robustness in AI systems\")\n",
    "else:\n",
    "    print(f\"\\nüéØ Keep trying! You need {90 - final_success_rate:.1f}% more success rate.\")\n",
    "    print(\"üí° Hint: Try even larger epsilon values or different attack strategies!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f64ad9",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è Step 8: Defense Awareness\n",
    "\n",
    "Now that you've successfully attacked a model, let's discuss defenses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18020ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üõ°Ô∏è Adversarial Defense Strategies:\")\n",
    "print(\"\\n1. Adversarial Training:\")\n",
    "print(\"   ‚Ä¢ Train models on adversarial examples\")\n",
    "print(\"   ‚Ä¢ Improves robustness but may reduce clean accuracy\")\n",
    "\n",
    "print(\"\\n2. Input Preprocessing:\")\n",
    "print(\"   ‚Ä¢ Gaussian noise addition\")\n",
    "print(\"   ‚Ä¢ JPEG compression\")\n",
    "print(\"   ‚Ä¢ Feature squeezing\")\n",
    "\n",
    "print(\"\\n3. Detection Methods:\")\n",
    "print(\"   ‚Ä¢ Statistical analysis of inputs\")\n",
    "print(\"   ‚Ä¢ Adversarial example detectors\")\n",
    "print(\"   ‚Ä¢ Ensemble methods\")\n",
    "\n",
    "print(\"\\n4. Certified Defenses:\")\n",
    "print(\"   ‚Ä¢ Randomized smoothing\")\n",
    "print(\"   ‚Ä¢ Interval bound propagation\")\n",
    "print(\"   ‚Ä¢ Formal verification\")\n",
    "\n",
    "print(\"\\nüîç Key Takeaways:\")\n",
    "print(\"‚Ä¢ Adversarial attacks are a fundamental security concern\")\n",
    "print(\"‚Ä¢ No perfect defense exists yet\")\n",
    "print(\"‚Ä¢ Defense-aware development is crucial\")\n",
    "print(\"‚Ä¢ Regular security testing should include adversarial robustness\")\n",
    "\n",
    "print(\"\\nüéì Congratulations on completing the AI Security Lab!\")\n",
    "print(\"You now understand the basics of adversarial attacks and defenses.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
