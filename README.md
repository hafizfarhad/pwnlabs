# AI Security Lab: Adversarial Evasion Attacks

Welcome to the interactive AI Security lab! This environment provides hands-on experience with adversarial attacks on image classifiers.

## 🎯 Lab Objectives

1. Understand how adversarial examples work
2. Implement Fast Gradient Sign Method (FGSM) attacks
3. Test attacks on pre-trained models
4. Analyze attack success rates and transferability

## 📚 What You'll Learn

- **Adversarial Examples**: Small perturbations that fool ML models
- **FGSM Attack**: Fast method for generating adversarial examples
- **Attack Evaluation**: Measuring attack effectiveness
- **Defense Awareness**: Understanding vulnerability patterns

## 🚀 Getting Started

1. Open `lab_notebook.ipynb` to begin the lab
2. Follow the step-by-step instructions
3. Run each cell and observe the results
4. Complete the challenges to earn your flag!

## 📁 Lab Files

- `lab_notebook.ipynb` - Main lab notebook with guided exercises
- `utils.py` - Helper functions for the lab
- `README.md` - This file

## 💡 Tips

- Take your time to understand each concept
- Experiment with different parameters
- Ask questions if you get stuck!

**Flag Format**: `PWNTHEPROMPT{...}`

Good luck and happy hacking! 🔐
