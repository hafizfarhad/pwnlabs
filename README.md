# AI Security Lab: Adversarial Evasion Attacks

Welcome to the interactive AI Security lab! This environment provides hands-on experience with adversarial attacks on image classifiers.

## ğŸ¯ Lab Objectives

1. Understand how adversarial examples work
2. Implement Fast Gradient Sign Method (FGSM) attacks
3. Test attacks on pre-trained models
4. Analyze attack success rates and transferability

## ğŸ“š What You'll Learn

- **Adversarial Examples**: Small perturbations that fool ML models
- **FGSM Attack**: Fast method for generating adversarial examples
- **Attack Evaluation**: Measuring attack effectiveness
- **Defense Awareness**: Understanding vulnerability patterns

## ğŸš€ Getting Started

1. Open `lab_notebook.ipynb` to begin the lab
2. Follow the step-by-step instructions
3. Run each cell and observe the results
4. Complete the challenges to earn your flag!

## ğŸ“ Lab Files

- `lab_notebook.ipynb` - Main lab notebook with guided exercises
- `utils.py` - Helper functions for the lab
- `README.md` - This file

## ğŸ’¡ Tips

- Take your time to understand each concept
- Experiment with different parameters
- Ask questions if you get stuck!

**Flag Format**: `PWNTHEPROMPT{...}`

Good luck and happy hacking! ğŸ”
